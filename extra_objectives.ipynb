{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddc4475e-2385-452b-a2dd-f4f16a26772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import gzip\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19176d52-2dbd-40af-9bea-480d16ec6850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6535, 0.9674, 0.5265],\n",
      "        [0.5251, 0.4700, 0.4561]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2,3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "134f7b45-9ebc-4f74-88a0-b931f054ce4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9674)\n"
     ]
    }
   ],
   "source": [
    "print(a[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e50397b-8f72-4721-a545-8777a8586d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, train_or_test, dataset_file, path_to_saved_embeddings=None):\n",
    "        super(SentimentAnalysisDataset, self).__init__()\n",
    "        import gensim.downloader as gen_api\n",
    "#                self.word_vectors = gen_api.load(\"word2vec-google-news-300\")\n",
    "        self.path_to_saved_embeddings = path_to_saved_embeddings\n",
    "        self.train_or_test = train_or_test\n",
    "        f = gzip.open(dataset_file, 'rb')\n",
    "        dataset = f.read()\n",
    "        if path_to_saved_embeddings is not None:\n",
    "            import gensim.downloader as genapi\n",
    "            from gensim.models import KeyedVectors \n",
    "            if os.path.exists(path_to_saved_embeddings + 'vectors.kv'):\n",
    "                self.word_vectors = KeyedVectors.load(path_to_saved_embeddings + 'vectors.kv')\n",
    "            else:\n",
    "                print(\"Downloading Embeddings\")\n",
    "                self.word_vectors = genapi.load(\"word2vec-google-news-300\")               \n",
    "                ##  'kv' stands for  \"KeyedVectors\", a special datatype used by gensim because it \n",
    "                ##  has a smaller footprint than dict\n",
    "                self.word_vectors.save(path_to_saved_embeddings + 'vectors.kv')    \n",
    "        if train_or_test == 'train':\n",
    "            if sys.version_info[0] == 3:\n",
    "                self.positive_reviews_train, self.negative_reviews_train, self.vocab = pickle.loads(dataset, encoding='latin1')\n",
    "            else:\n",
    "                self.positive_reviews_train, self.negative_reviews_train, self.vocab = pickle.loads(dataset)\n",
    "            self.categories = sorted(list(self.positive_reviews_train.keys()))\n",
    "            self.category_sizes_train_pos = {category : len(self.positive_reviews_train[category]) for category in self.categories}\n",
    "            self.category_sizes_train_neg = {category : len(self.negative_reviews_train[category]) for category in self.categories}\n",
    "            self.indexed_dataset_train = []\n",
    "            for category in self.positive_reviews_train:\n",
    "                for review in self.positive_reviews_train[category]:\n",
    "                    self.indexed_dataset_train.append([review, category, 1])\n",
    "            for category in self.negative_reviews_train:\n",
    "                for review in self.negative_reviews_train[category]:\n",
    "                    self.indexed_dataset_train.append([review, category, 0])\n",
    "            random.shuffle(self.indexed_dataset_train)\n",
    "        elif train_or_test == 'test':\n",
    "            if sys.version_info[0] == 3:\n",
    "                self.positive_reviews_test, self.negative_reviews_test, self.vocab = pickle.loads(dataset, encoding='latin1')\n",
    "            else:\n",
    "                self.positive_reviews_test, self.negative_reviews_test, self.vocab = pickle.loads(dataset)\n",
    "            self.vocab = sorted(self.vocab)\n",
    "            self.categories = sorted(list(self.positive_reviews_test.keys()))\n",
    "            self.category_sizes_test_pos = {category : len(self.positive_reviews_test[category]) for category in self.categories}\n",
    "            self.category_sizes_test_neg = {category : len(self.negative_reviews_test[category]) for category in self.categories}\n",
    "            self.indexed_dataset_test = []\n",
    "            for category in self.positive_reviews_test:\n",
    "                for review in self.positive_reviews_test[category]:\n",
    "                    self.indexed_dataset_test.append([review, category, 1])\n",
    "            for category in self.negative_reviews_test:\n",
    "                for review in self.negative_reviews_test[category]:\n",
    "                    self.indexed_dataset_test.append([review, category, 0])\n",
    "            random.shuffle(self.indexed_dataset_test)\n",
    "\n",
    "    def review_to_tensor(self, review):\n",
    "        list_of_embeddings = []\n",
    "        for i,word in enumerate(review):\n",
    "            if word in self.word_vectors.key_to_index:\n",
    "                embedding = self.word_vectors[word]\n",
    "                list_of_embeddings.append(np.array(embedding))\n",
    "            else:\n",
    "                next\n",
    "#                review_tensor = torch.FloatTensor( list_of_embeddings )\n",
    "        review_tensor = torch.FloatTensor( np.array(list_of_embeddings) )\n",
    "        return review_tensor\n",
    "\n",
    "    def sentiment_to_tensor(self, sentiment):\n",
    "         \n",
    "        sentiment_tensor = torch.zeros(2)\n",
    "        if sentiment == 1:\n",
    "            sentiment_tensor[1] = 1\n",
    "        elif sentiment == 0: \n",
    "            sentiment_tensor[0] = 1\n",
    "        sentiment_tensor = sentiment_tensor.type(torch.long)\n",
    "        return sentiment_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train_or_test == 'train':\n",
    "            return len(self.indexed_dataset_train)\n",
    "        elif self.train_or_test == 'test':\n",
    "            return len(self.indexed_dataset_test)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.indexed_dataset_train[idx] if self.train_or_test == 'train' else self.indexed_dataset_test[idx]\n",
    "        review = sample[0]\n",
    "        review_category = sample[1]\n",
    "        review_sentiment = sample[2]\n",
    "        review_sentiment = self.sentiment_to_tensor(review_sentiment)\n",
    "        review_tensor = self.review_to_tensor(review)\n",
    "        category_index = self.categories.index(review_category)\n",
    "        sample = {'review'       : review_tensor, \n",
    "                  'category'     : category_index, # should be converted to tensor, but not yet used\n",
    "                  'sentiment'    : review_sentiment }\n",
    "        return sample\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cedac4f-a16b-491b-829e-1ebe947de5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GRUnetWithEmbeddings(nn.Module):\n",
    "#             \"\"\"\n",
    "#             For this embeddings adapted version of the GRUnet shown earlier, we can assume that\n",
    "#             the 'input_size' for a tensor representing a word is always 300.\n",
    "#             Source: https://blog.floydhub.com/gru-with-pytorch/\n",
    "#             with the only modification that the final output of forward() is now\n",
    "#             routed through LogSoftmax activation. \n",
    "\n",
    "#             Class Path:  DLStudio -> TextClassificationWithEmbeddings -> GRUnetWithEmbeddings \n",
    "#             \"\"\"\n",
    "#             def __init__(self, input_size, hidden_size, output_size, num_layers=1): \n",
    "#                 \"\"\"\n",
    "#                 -- input_size is the size of the tensor for each word in a sequence of words.  If you word2vec\n",
    "#                        embedding, the value of this variable will always be equal to 300.\n",
    "#                 -- hidden_size is the size of the hidden state in the RNN\n",
    "#                 -- output_size is the size of output of the RNN.  For binary classification of \n",
    "#                        input text, output_size is 2.\n",
    "#                 -- num_layers creates a stack of GRUs\n",
    "#                 \"\"\"\n",
    "#                 super(DLStudio.TextClassificationWithEmbeddings.GRUnetWithEmbeddings, self).__init__()\n",
    "#                 self.input_size = input_size\n",
    "#                 self.hidden_size = hidden_size\n",
    "#                 self.num_layers = num_layers\n",
    "#                 self.gru = nn.GRU(input_size, hidden_size, num_layers)\n",
    "#                 self.fc = nn.Linear(hidden_size, output_size)\n",
    "#                 self.relu = nn.ReLU()\n",
    "#                 self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "                \n",
    "#             def forward(self, x, h):\n",
    "#                 out, h = self.gru(x, h)\n",
    "#                 out = self.fc(self.relu(out[:,-1]))\n",
    "#                 out = self.logsoftmax(out)\n",
    "#                 return out, h\n",
    "\n",
    "#             def init_hidden(self):\n",
    "#                 weight = next(self.parameters()).data\n",
    "#                 #                  num_layers  batch_size    hidden_size\n",
    "#                 hidden = weight.new(  2,          1,         self.hidden_size    ).zero_()\n",
    "#                 return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39cd0e52-2c7d-4b10-9fb1-8a2ce1ec356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_code_for_training_for_text_classification_with_GRU_word2vec(net,train_dataloader, epochs = 5, learning_rate = 1e-3, momentum = 0.9, display_train_loss=False): \n",
    "    filename_for_out = \"performance_numbers_\" + str(epochs) + \".txt\"\n",
    "    FILE = open(filename_for_out, 'w')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net = net.to(device)\n",
    "    ##  Note that the GRUnet now produces the LogSoftmax output:\n",
    "    criterion = nn.NLLLoss()\n",
    "    accum_times = []\n",
    "    optimizer = torch.optim.SGD(net.parameters(), \n",
    "                 lr=learning_rate, momentum=momentum)\n",
    "    training_loss_tally = []\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch in range(epochs):  \n",
    "        print(\"\")\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_dataloader):    \n",
    "            review_tensor,category,sentiment = data['review'], data['category'], data['sentiment']\n",
    "            review_tensor = review_tensor.to(device)\n",
    "            sentiment = sentiment.to(device)\n",
    "            ## The following type conversion needed for MSELoss:\n",
    "            ##sentiment = sentiment.float()\n",
    "            optimizer.zero_grad()\n",
    "            hidden = net.init_hidden().to(device)\n",
    "            for k in range(review_tensor.shape[1]):\n",
    "                output, hidden = net(torch.unsqueeze(torch.unsqueeze(review_tensor[0,k],0),0), hidden)\n",
    "            loss = criterion(output, torch.argmax(sentiment, 1))\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 200 == 199:    \n",
    "                avg_loss = running_loss / float(200)\n",
    "                training_loss_tally.append(avg_loss)\n",
    "                current_time = time.perf_counter()\n",
    "                time_elapsed = current_time-start_time\n",
    "                print(\"[epoch:%d  iter:%4d  elapsed_time:%4d secs]     loss: %.5f\" % (epoch+1,i+1, time_elapsed,avg_loss))\n",
    "                accum_times.append(current_time-start_time)\n",
    "                FILE.write(\"%.5f\\n\" % avg_loss)\n",
    "                FILE.flush()\n",
    "                running_loss = 0.0\n",
    "    print(\"Total Training Time: {}\".format(str(sum(accum_times))))\n",
    "    print(\"\\nFinished Training\\n\\n\")\n",
    "    if display_train_loss:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.title(\"Training Loss vs. Iterations\")\n",
    "        plt.plot(training_loss_tally)\n",
    "        plt.xlabel(\"iterations\")\n",
    "        plt.ylabel(\"training loss\")\n",
    "#                plt.legend()\n",
    "        plt.legend([\"Plot of loss versus iterations\"], fontsize=\"x-large\")\n",
    "        plt.savefig(\"training_loss.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28409faa-1925-4657-9103-a950d6cd99b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_code_for_testing_text_classification_with_GRU_word2vec(net, test_dataloader):\n",
    "        classification_accuracy = 0.0\n",
    "        negative_total = 0\n",
    "        positive_total = 0\n",
    "        confusion_matrix = torch.zeros(2,2)\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_dataloader):\n",
    "                review_tensor,category,sentiment = data['review'], data['category'], data['sentiment']\n",
    "                hidden = net.init_hidden()\n",
    "                for k in range(review_tensor.shape[1]):\n",
    "                    output, hidden = net(torch.unsqueeze(torch.unsqueeze(review_tensor[0,k],0),0), hidden)\n",
    "                predicted_idx = torch.argmax(output).item()\n",
    "                gt_idx = torch.argmax(sentiment).item()\n",
    "                if i % 100 == 99:\n",
    "                    print(\"   [i=%d]    predicted_label=%d       gt_label=%d\" % (i+1, predicted_idx,gt_idx))\n",
    "                if predicted_idx == gt_idx:\n",
    "                    classification_accuracy += 1\n",
    "                if gt_idx == 0: \n",
    "                    negative_total += 1\n",
    "                elif gt_idx == 1:\n",
    "                    positive_total += 1\n",
    "                confusion_matrix[gt_idx,predicted_idx] += 1\n",
    "        print(\"\\nOverall classification accuracy: %0.2f%%\" %  (float(classification_accuracy) * 100 /float(i)))\n",
    "        out_percent = np.zeros((2,2), dtype='float')\n",
    "        out_percent[0,0] = \"%.3f\" % (100 * confusion_matrix[0,0] / float(negative_total))\n",
    "        out_percent[0,1] = \"%.3f\" % (100 * confusion_matrix[0,1] / float(negative_total))\n",
    "        out_percent[1,0] = \"%.3f\" % (100 * confusion_matrix[1,0] / float(positive_total))\n",
    "        out_percent[1,1] = \"%.3f\" % (100 * confusion_matrix[1,1] / float(positive_total))\n",
    "        print(\"\\n\\nNumber of positive reviews tested: %d\" % positive_total)\n",
    "        print(\"\\n\\nNumber of negative reviews tested: %d\" % negative_total)\n",
    "        print(\"\\n\\nDisplaying the confusion matrix:\\n\")\n",
    "        out_str = \"                      \"\n",
    "        out_str +=  \"%18s    %18s\" % ('predicted negative', 'predicted positive')\n",
    "        print(out_str + \"\\n\")\n",
    "        for i,label in enumerate(['true negative', 'true positive']):\n",
    "            out_str = \"%12s:  \" % label\n",
    "            for j in range(2):\n",
    "                out_str +=  \"%18s%%\" % out_percent[i,j]\n",
    "            print(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eb8ea7a-66c7-4400-8c21-8e00696287e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m dataset_archive_test_400 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment_dataset_test_400.tar.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m path_to_saved_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m dataserver_train_200 \u001b[38;5;241m=\u001b[39m \u001b[43mSentimentAnalysisDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mtrain_or_test\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mdataset_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdataset_archive_train_200\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpath_to_saved_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpath_to_saved_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                   \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m dataserver_test_200 \u001b[38;5;241m=\u001b[39m SentimentAnalysisDataset(\n\u001b[0;32m     13\u001b[0m                                  train_or_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     14\u001b[0m                                  dataset_file \u001b[38;5;241m=\u001b[39m dataset_archive_test_200,\n\u001b[0;32m     15\u001b[0m                                  path_to_saved_embeddings \u001b[38;5;241m=\u001b[39m path_to_saved_embeddings,\n\u001b[0;32m     16\u001b[0m                   )\n",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m, in \u001b[0;36mSentimentAnalysisDataset.__init__\u001b[1;34m(self, train_or_test, dataset_file, path_to_saved_embeddings)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_or_test, dataset_file, path_to_saved_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28msuper\u001b[39m(SentimentAnalysisDataset, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m---> 11\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgen_api\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#                self.word_vectors = gen_api.load(\"word2vec-google-news-300\")\u001b[39;00m\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_to_saved_embeddings \u001b[38;5;241m=\u001b[39m path_to_saved_embeddings\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "dataset_archive_train_200 = \"sentiment_dataset_train_200.tar.gz\"\n",
    "dataset_archive_test_200 = \"sentiment_dataset_test_200.tar.gz\"\n",
    "dataset_archive_train_400 = \"sentiment_dataset_train_400.tar.gz\"\n",
    "dataset_archive_test_400 = \"sentiment_dataset_test_400.tar.gz\"\n",
    "path_to_saved_embeddings = \"\"\n",
    "\n",
    "dataserver_train_200 = SentimentAnalysisDataset(\n",
    "                                 train_or_test = 'train',\n",
    "                                 dataset_file = dataset_archive_train_200,\n",
    "                                 path_to_saved_embeddings = path_to_saved_embeddings,\n",
    "                   )\n",
    "dataserver_test_200 = SentimentAnalysisDataset(\n",
    "                                 train_or_test = 'test',\n",
    "                                 dataset_file = dataset_archive_test_200,\n",
    "                                 path_to_saved_embeddings = path_to_saved_embeddings,\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a936fb69-895c-4c38-b8dc-b319515748ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_dataloader_200 = torch.utils.data.DataLoader(dataserver_train_200,\n",
    "                batch_size=batch_size,shuffle=True, num_workers=2)\n",
    "test_dataloader_200 = torch.utils.data.DataLoader(dataserver_test_200,\n",
    "                       batch_size=batch_size,shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea4c9ce-d6e2-4fd4-b8ad-b7e84d0853f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUnet(torch.nn.Module):\n",
    "            \"\"\"\n",
    "            Source: https://blog.floydhub.com/gru-with-pytorch/\n",
    "            with the only modification that the final output of forward() is now\n",
    "            routed through LogSoftmax activation.\n",
    "\n",
    "            Class Path: DLStudio  ->  TextClassification  ->  GRUnet\n",
    "            \"\"\"\n",
    "            def __init__(self, input_size, hidden_size, output_size, num_layers,  batch_size, drop_prob=0.2):\n",
    "                super(GRUnet, self).__init__()\n",
    "                self.hidden_size = hidden_size\n",
    "                self.num_layers = num_layers\n",
    "                self.gru = nn.GRU(input_size, hidden_size, num_layers,batch_first=True, dropout=drop_prob)\n",
    "                self.fc = nn.Linear(hidden_size, output_size)\n",
    "                self.relu = nn.ReLU()\n",
    "                self.softmax = nn.Softmax(dim=1)\n",
    "                self.batch_size = batch_size\n",
    "\n",
    "            def forward(self, x, h):\n",
    "                out, h = self.gru(x, h)\n",
    "                out = self.fc(self.relu(out[:,-1]))\n",
    "                out = self.softmax(out)\n",
    "                return out, h\n",
    "\n",
    "            def init_hidden(self):\n",
    "                weight = next(self.parameters()).data\n",
    "                #                                     batch_size\n",
    "                hidden = weight.new(  self.num_layers,    self.batch_size,         self.hidden_size   ).zero_()\n",
    "                return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98ac679-9f72-426e-b75a-bc2d71f7e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGRUnet(torch.nn.Module):\n",
    "            \"\"\"\n",
    "            Source: https://blog.floydhub.com/gru-with-pytorch/\n",
    "            with the only modification that the final output of forward() is now\n",
    "            routed through LogSoftmax activation.\n",
    "\n",
    "            Class Path: DLStudio  ->  TextClassification  ->  GRUnet\n",
    "            \"\"\"\n",
    "            def __init__(self, input_size, hidden_size, output_size, num_layers,  batch_size, drop_prob=0.2):\n",
    "                super(BiGRUnet, self).__init__()\n",
    "                self.hidden_size = hidden_size\n",
    "                self.num_layers = num_layers\n",
    "                self.gru = nn.GRU(input_size, hidden_size, num_layers,batch_first=True, dropout=drop_prob, bidirectional=True)\n",
    "                self.fc = nn.Linear(2*hidden_size, output_size)\n",
    "                self.relu = nn.ReLU()\n",
    "                self.softmax = nn.Softmax(dim=1)\n",
    "                self.batch_size = batch_size\n",
    "\n",
    "            def forward(self, x, h):\n",
    "                out, h = self.gru(x, h)\n",
    "                out = self.fc(self.relu(out[:,-1]))\n",
    "                out = self.softmax(out)\n",
    "                return out, h\n",
    "\n",
    "            def init_hidden(self):\n",
    "                weight = next(self.parameters()).data\n",
    "                #                                     batch_size\n",
    "                hidden = weight.new(  2*self.num_layers,    self.batch_size,         self.hidden_size   ).zero_()\n",
    "                return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964671d9-9e79-4526-a981-ca56414191e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_200 = GRUnet(input_size = 300, hidden_size = 100,output_size = 2, num_layers = 2, batch_size = 8)\n",
    "gru_400 = GRUnet(input_size = 300, hidden_size = 100,output_size = 2, num_layers = 2, batch_size = 8)\n",
    "bi_gru_200 = BiGRUnet(input_size = 300, hidden_size = 100,output_size = 2, num_layers = 2, batch_size = 8)\n",
    "bi_gru_400 = BiGRUnet(input_size = 300, hidden_size = 100,output_size = 2, num_layers = 2, batch_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f5cd3b-3bc2-4c82-890a-6b645560b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_code_for_training_for_text_classification_with_GRU_word2vec(gru_200,train_dataloader_200, display_train_loss=False)\n",
    "run_code_for_testing_text_classification_with_GRU_word2vec(gru_200, test_dataloader_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ac5cfa-5c1c-48f7-9a20-809e52ea9a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_code_for_training_for_text_classification_with_GRU_word2vec(bi_gru_200,train_dataloader_200, display_train_loss=False)\n",
    "run_code_for_testing_text_classification_with_GRU_word2vec(bi_gru_200, test_dataloader_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a714b3af-f5c0-4fb9-a405-b04e447506c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataserver_train_400 = SentimentAnalysisDataset(\n",
    "                                 train_or_test = 'train',\n",
    "                                 dataset_file = dataset_archive_train_200,\n",
    "                                 path_to_saved_embeddings = path_to_saved_embeddings,\n",
    "                   )\n",
    "dataserver_test_400 = SentimentAnalysisDataset(\n",
    "                                 train_or_test = 'test',\n",
    "                                 dataset_file = dataset_archive_test_200,\n",
    "                                 path_to_saved_embeddings = path_to_saved_embeddings,\n",
    "                  )\n",
    "train_dataloader_400 = torch.utils.data.DataLoader(dataserver_train_400,\n",
    "                batch_size=batch_size,shuffle=True, num_workers=2)\n",
    "test_dataloader_400 = torch.utils.data.DataLoader(dataserver_test_400,\n",
    "                       batch_size=batch_size,shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d94ed-4a1e-48df-9bc8-7a6f091855af",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_code_for_training_for_text_classification_with_GRU_word2vec(gru_400,train_dataloader_400, display_train_loss=False)\n",
    "run_code_for_testing_text_classification_with_GRU_word2vec(gru_400, test_dataloader_400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1423722d-008b-4701-80bf-7e33855ed3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_code_for_training_for_text_classification_with_GRU_word2vec(bi_gru_400,train_dataloader_400, display_train_loss=False)\n",
    "run_code_for_testing_text_classification_with_GRU_word2vec(bi_gru_400, test_dataloader_400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64320d3-8427-4032-9a73-09904dd3cf33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
